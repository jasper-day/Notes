{
  "hash": "f5dba5f37230ad5d0e7275caa52c82b7",
  "result": {
    "markdown": "---\ntitle: \"Convolutions\"\nauthor: \"Jasper Day\"\ndate: \"6/6/2022\"\n---\n\n# Kernels and Convolutions\n\nStart with a blur: \n\nFor each pixel, take the average of the pixels around it (or a weighted sum of the pixels around it.)\n\nThe resulting values are a convolution of two grids (combining two functions to get a new function.) A blur would be a convolution with a *gaussian kernel*: take a larger image, and replace the value of each pixel with the sum of the values in the kernel times the values of the corresponding surrounding pixels.\n\n$$\nK = \\begin{bmatrix} \n    0.003 & 0.013 & 0.022 & 0.013 & 0.003 \\\\\n    0.013 & 0.060 & 0.098 & 0.060 & 0.013 \\\\\n    0.022 & 0.098 & 0.162 & 0.098 & 0.022 \\\\\n    0.013 & 0.060 & 0.098 & 0.060 & 0.013 \\\\\n    0.003 & 0.013 & 0.022 & 0.013 & 0.003\n    \\end{bmatrix}\n$${#eq-gauss}\n\n@eq-gauss shows a sample 5x5 gaussian kernel.\n\nThe gaussian is a particularly nice curve to take since the sum of the area under the curve is 1.\n\nLet's start coding up some examples:\n\n::: {.cell execution_count=1}\n``` {.julia .cell-code}\nusing Images\nusing Plots\n```\n:::\n\n\nHere's the image we'll use today.\n\n[![Cool cat courtesy of Raoul Droog on Unsplash](/coolcat.jpg)](https://unsplash.com/photos/yMSecCHsIBc){height=50%}\n\n::: {.cell execution_count=2}\n``` {.julia .cell-code}\ndownload(\"https://unsplash.com/photos/yMSecCHsIBc/download?ixid=MnwxMjA3fDB8MXxzZWFyY2h8NXx8Y2F0fGVufDB8fHx8MTY1NDUxNzk5MA&force=true&w=640\", \"./coolcat.jpg\")\ncoolcat = load(\"./coolcat.jpg\")\n@show size(coolcat);\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nsize(coolcat) = (853, 640)\n```\n:::\n:::\n\n\nNow let's make a gaussian:\n\n::: {.cell execution_count=3}\n``` {.julia .cell-code}\nkernel = Kernel.gaussian((1,1))\n```\n\n::: {.cell-output .cell-output-display execution_count=4}\n```\n5×5 OffsetArray(::Matrix{Float64}, -2:2, -2:2) with eltype Float64 with indices -2:2×-2:2:\n 0.00296902  0.0133062  0.0219382  0.0133062  0.00296902\n 0.0133062   0.0596343  0.0983203  0.0596343  0.0133062\n 0.0219382   0.0983203  0.162103   0.0983203  0.0219382\n 0.0133062   0.0596343  0.0983203  0.0596343  0.0133062\n 0.00296902  0.0133062  0.0219382  0.0133062  0.00296902\n```\n:::\n:::\n\n\n\"`5x5 OffsetArray`\" means that `[0,0]` is actually in the center of the array.\n\nHere's what the kernel looks like at a couple of different sizes:\n\n::: {.cell execution_count=4}\n``` {.julia .cell-code}\nheatmaps = []\nfor i in 1:6\n    kerneli = Kernel.gaussian((i,i))\n    bounds = size(kerneli)[1] ÷ 2\n    boundsrange = -bounds:bounds\n    push!(heatmaps, heatmap(boundsrange,boundsrange,kerneli, legend=false))\nend\nplot(heatmaps..., layout =(3,2))\n```\n\n::: {.cell-output .cell-output-display execution_count=5}\n![](Convolutions_files/figure-html/cell-5-output-1.svg){}\n:::\n:::\n\n\nNote that the sum of the kernel is 1, so our image shouldn't get any lighter or darker under the convolution.\n\nWhen you convolve the kernel and the image, the kernel \"marches across\" the image, replacing each value with the elementwise multiplication of the values in the kernel with the corresponding values in the image.\n\nUsing a kernel that adds the values around the center to the center gives a blurrier image (averaging the pixels around it with the center pixel). Using a kernel that subtracts the values of the surrounding pixels gives a sharper image.\n\n# The Convolution Function\n\nWe need a convolution function. That\n\n# Edge detection\n\nTo do edge detection, use a kernel that subtracts the top half from the bottom half, or vice versa.\n\n::: {.cell execution_count=5}\n``` {.julia .cell-code}\n@show horEdgeKern = Kernel.sobel()[1];\n@show vertEdgeKern = Kernel.sobel()[2];\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nhorEdgeKern = (Kernel.sobel())[1] = [-0.125 -0.25 -0.125; 0.0 0.0 0.0; 0.125 0.25 0.125]\nvertEdgeKern = (Kernel.sobel())[2] = [-0.125 0.0 0.125; -0.25 0.0 0.25; -0.125 0.0 0.125]\n```\n:::\n:::\n\n\nThe horizontal edge detection kernel looks like this:\n\n$$\nK = \\begin{bmatrix}\n    -0.125 & 0.0 & 0.125 \\\\\n    -0.25 & 0.0 & 0.25 \\\\\n    -0.125 & 0.0 & 0.125 \\\\\n\\end{bmatrix}\n$$\n\nAnd the vertical edge detection kernel is the transpose of the horizontal.\n\nWhen you get values that change above and below the center of the kernel, the edge detection kernels return either a positive or a negative number.\n\nA *convolutional neural network* uses a machine to learn which kernel it should use to get the desired output.\n\nIn the mathematical context, you generally rotate the convolutional kernel 180° before you apply it.\n\n# Polynomial Multiplication\n\nConsider:\n\n$$\n\\left( k_0 + k_1x + k_2x^2\\right)\\left(a_0 + a_1x + a_2x^2 + a_3x^3 + a_4x^4 \\cdots \\right)\n$$\n\nThe constant for the $x^3$ term in this expression is \n\n$$\nk_2a_1 + k_1a_2 + k_0a_3\n$${#eq-x3}\n\nThe total multiplication is:\n\n$$\n\\begin{split}\n(k_0a_0)x^0 &+\\\\\n(k_1a_0 + k_0a_1)x^1 &+\\\\\n(k_2a_0 + k_1a_1 + k_0a_2)x^2 &+\\\\\n(k_2a_1 + k_1a_2 + k_0a_3)x^3 &+\\\\\n(k_2a_2 + k_1a_3 + k_0a_4)x^4 &+\\\\\n(k_2a_3 + k_1a_4 + k_0a_5)x^5 &+\\\\\n&\\vdots \\end{split}\n$${#eq-polymul}\n\nYou could describe the polynomial multiplication in @eq-polymul as a *convolution* between the terms of $\\left( k_0 + k_1x + k_2x^2\\right)$ and $\\left(a_0 + a_1x + a_2x^2 + a_3x^3 + a_4x^4 \\cdots \\right)$! But in order for that to work, you first have to rotate the $k$ polynomial by 180 degrees, so that $k_2x^2$ comes first and $k_0$ comes last.\n\nThis becomes a convention.\n\nPixel data isn't actually stored as a multiple of powers of $x$, but it *is* common to see pixel data as a fourier transform, in which case you would be multiplying the kernel by\n\n$$\n\\left(\n    a_0 \n    + a_1\\left(e^{2\\pi i f}\\right) \n    + a_2\\left(e^{2\\pi i f}\\right)^2\n    + a_3\\left(e^{2\\pi i f}\\right)^3\n    + a_4\\left(e^{2\\pi i f}\\right)^4\n    + \\cdots\n\\right)\n$${#eq-fourier}\n\n\n:::{.remark}\nNote about fourier transforms: They're very analogous to power series, in that we're multiplying by something we have successive powers of. When we multiply two fourier series, the result corresponds to convolving the original terms of the series.\n:::\n\nSo we can take the fourier transforms of our image and our kernel, multiply them together, and detransform. The multiplication of the two fourier transforms is equivalent to convolving by the (rotated) kernel.\n\n# Takeaways\n\n* A simple tool that seems specific (taking the moving averages of the pixel values in an image) can be split into the general idea (in this case convolution), which opens up a lot more room to play.\n    * Blurring, sharpening, edge detecting\n* Representing the same data, or the same operations, different ways in different domains can allow you to solve problems much faster.\n\n",
    "supporting": [
      "Convolutions_files"
    ],
    "filters": [],
    "includes": {}
  }
}